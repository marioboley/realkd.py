{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pclass' 'Age' 'SibSp' 'Parch' 'Fare']\n",
      "[[  1.      38.       1.       0.      71.2833]\n",
      " [  1.      35.       1.       0.      53.1   ]\n",
      " [  1.      54.       0.       0.      51.8625]\n",
      " [  3.       4.       1.       1.      16.7   ]\n",
      " [  1.      58.       0.       0.      26.55  ]\n",
      " [  2.      34.       0.       0.      13.    ]\n",
      " [  1.      28.       0.       0.      35.5   ]\n",
      " [  1.      19.       3.       2.     263.    ]\n",
      " [  1.      49.       1.       0.      76.7292]\n",
      " [  1.      65.       0.       1.      61.9792]\n",
      " [  1.      45.       1.       0.      83.475 ]\n",
      " [  2.      29.       0.       0.      10.5   ]\n",
      " [  3.      25.       0.       0.       7.65  ]\n",
      " [  1.      23.       3.       2.     263.    ]\n",
      " [  1.      46.       1.       0.      61.175 ]\n",
      " [  1.      71.       0.       0.      34.6542]\n",
      " [  1.      23.       0.       1.      63.3583]\n",
      " [  1.      21.       0.       1.      77.2875]\n",
      " [  1.      47.       0.       0.      52.    ]\n",
      " [  1.      24.       0.       1.     247.5208]\n",
      " [  2.      32.5      0.       0.      13.    ]\n",
      " [  1.      54.       0.       1.      77.2875]\n",
      " [  1.      19.       0.       2.      26.2833]\n",
      " [  1.      37.       1.       0.      53.1   ]\n",
      " [  1.      24.       0.       0.      79.2   ]\n",
      " [  2.      36.5      0.       2.      26.    ]\n",
      " [  1.      22.       1.       0.      66.6   ]\n",
      " [  1.      61.       0.       0.      33.5   ]\n",
      " [  1.      56.       0.       0.      30.6958]\n",
      " [  1.      50.       0.       0.      28.7125]\n",
      " [  2.       1.       2.       1.      39.    ]\n",
      " [  2.       3.       1.       1.      26.    ]\n",
      " [  1.      44.       0.       0.      27.7208]\n",
      " [  1.      58.       0.       0.     146.5208]\n",
      " [  3.       2.       0.       1.      10.4625]\n",
      " [  1.      40.       0.       0.      31.    ]\n",
      " [  1.      31.       1.       0.     113.275 ]\n",
      " [  1.      32.       0.       0.      76.2917]\n",
      " [  1.      38.       1.       0.      90.    ]\n",
      " [  1.      35.       1.       0.      83.475 ]\n",
      " [  1.      44.       2.       0.      90.    ]\n",
      " [  1.      37.       1.       1.      52.5542]\n",
      " [  3.      29.       1.       1.      10.4625]\n",
      " [  1.      62.       0.       0.      26.55  ]\n",
      " [  1.      30.       0.       0.      86.5   ]\n",
      " [  1.      52.       1.       1.      79.65  ]\n",
      " [  1.      40.       0.       0.       0.    ]\n",
      " [  1.      58.       0.       1.     153.4625]\n",
      " [  1.      35.       0.       0.     135.6333]\n",
      " [  1.      37.       0.       1.      29.7   ]\n",
      " [  1.      63.       1.       0.      77.9583]\n",
      " [  1.      19.       1.       0.      91.0792]\n",
      " [  2.      36.       0.       0.      12.875 ]\n",
      " [  1.       2.       1.       2.     151.55  ]\n",
      " [  1.      50.       0.       1.     247.5208]\n",
      " [  1.       0.92     1.       2.     151.55  ]\n",
      " [  1.      17.       1.       0.     108.9   ]\n",
      " [  1.      30.       0.       0.      56.9292]\n",
      " [  1.      24.       0.       0.      83.1583]\n",
      " [  1.      18.       2.       2.     262.375 ]\n",
      " [  1.      31.       0.       2.     164.8667]\n",
      " [  1.      40.       1.       1.     134.5   ]\n",
      " [  1.      36.       0.       0.     135.6333]\n",
      " [  2.      36.       0.       0.      13.    ]\n",
      " [  1.      16.       0.       1.      57.9792]\n",
      " [  1.      45.5      0.       0.      28.5   ]\n",
      " [  1.      38.       0.       1.     153.4625]\n",
      " [  1.      29.       1.       0.      66.6   ]\n",
      " [  1.      41.       0.       0.     134.5   ]\n",
      " [  1.      45.       0.       0.      35.5   ]\n",
      " [  2.       2.       1.       1.      26.    ]\n",
      " [  1.      24.       3.       2.     263.    ]\n",
      " [  2.      24.       0.       0.      13.    ]\n",
      " [  1.      22.       0.       1.      55.    ]\n",
      " [  1.      60.       1.       0.      75.25  ]\n",
      " [  1.      24.       0.       0.      69.3   ]\n",
      " [  1.      25.       1.       0.      55.4417]\n",
      " [  1.      27.       0.       2.     211.5   ]\n",
      " [  1.      36.       1.       2.     120.    ]\n",
      " [  1.      23.       1.       0.     113.275 ]\n",
      " [  3.      24.       0.       2.      16.7   ]\n",
      " [  1.      33.       1.       0.      90.    ]\n",
      " [  3.      32.       0.       0.       8.05  ]\n",
      " [  1.      28.       0.       0.      26.55  ]\n",
      " [  1.      50.       1.       0.      55.9   ]\n",
      " [  1.      14.       1.       2.     120.    ]\n",
      " [  1.      64.       1.       4.     263.    ]\n",
      " [  1.       4.       0.       2.      81.8583]\n",
      " [  1.      52.       0.       0.      30.5   ]\n",
      " [  1.      30.       0.       0.      27.75  ]\n",
      " [  1.      49.       1.       0.      89.1042]\n",
      " [  1.      65.       0.       0.      26.55  ]\n",
      " [  1.      48.       0.       0.      26.55  ]\n",
      " [  1.      47.       0.       0.      38.5   ]\n",
      " [  2.      23.       0.       0.      13.7917]\n",
      " [  1.      25.       1.       0.      91.0792]\n",
      " [  1.      35.       1.       0.      90.    ]\n",
      " [  1.      58.       0.       0.      29.7   ]\n",
      " [  1.      55.       0.       0.      30.5   ]\n",
      " [  1.      54.       1.       0.      78.2667]\n",
      " [  1.      25.       1.       2.     151.55  ]\n",
      " [  1.      16.       0.       0.      86.5   ]\n",
      " [  1.      18.       1.       0.     108.9   ]\n",
      " [  1.      36.       0.       0.      26.2875]\n",
      " [  1.      47.       0.       0.      34.0208]\n",
      " [  2.      34.       0.       0.      10.5   ]\n",
      " [  1.      30.       0.       0.      93.5   ]\n",
      " [  1.      44.       0.       1.      57.9792]\n",
      " [  1.      45.       0.       0.      26.55  ]\n",
      " [  1.      22.       0.       2.      49.5   ]\n",
      " [  1.      36.       0.       2.      71.    ]\n",
      " [  1.      50.       1.       0.     106.425 ]\n",
      " [  1.      17.       0.       2.     110.8833]\n",
      " [  1.      48.       1.       0.      39.6   ]\n",
      " [  1.      39.       1.       1.      79.65  ]\n",
      " [  1.      53.       2.       0.      51.4792]\n",
      " [  1.      36.       0.       0.      26.3875]\n",
      " [  1.      39.       1.       0.      55.9   ]\n",
      " [  1.      39.       1.       1.     110.8833]\n",
      " [  1.      36.       0.       0.      40.125 ]\n",
      " [  1.      18.       0.       2.      79.65  ]\n",
      " [  1.      60.       1.       1.      79.2   ]\n",
      " [  1.      52.       1.       0.      78.2667]\n",
      " [  1.      49.       1.       0.      56.9292]\n",
      " [  1.      40.       0.       0.     153.4625]\n",
      " [  2.       4.       2.       1.      39.    ]\n",
      " [  1.      42.       1.       0.      52.5542]\n",
      " [  1.      61.       0.       0.      32.3208]\n",
      " [  1.      21.       0.       0.      77.9583]\n",
      " [  1.      80.       0.       0.      30.    ]\n",
      " [  1.      32.       0.       0.      30.5   ]\n",
      " [  1.      24.       0.       0.      69.3   ]\n",
      " [  1.      48.       1.       0.      76.7292]\n",
      " [  1.      56.       0.       0.      35.5   ]\n",
      " [  1.      58.       0.       2.     113.275 ]\n",
      " [  1.      47.       0.       0.      25.5875]\n",
      " [  1.      31.       1.       0.      52.    ]\n",
      " [  1.      36.       0.       1.     512.3292]\n",
      " [  1.      27.       0.       0.      76.7292]\n",
      " [  1.      15.       0.       1.     211.3375]\n",
      " [  1.      31.       1.       0.      57.    ]\n",
      " [  1.      49.       1.       1.     110.8833]\n",
      " [  3.      42.       0.       0.       7.65  ]\n",
      " [  1.      18.       1.       0.     227.525 ]\n",
      " [  1.      35.       0.       0.      26.2875]\n",
      " [  1.      42.       0.       0.      26.2875]\n",
      " [  1.      24.       0.       0.      49.5042]\n",
      " [  1.      48.       1.       0.      52.    ]\n",
      " [  3.      19.       0.       0.       7.65  ]\n",
      " [  1.      38.       0.       0.     227.525 ]\n",
      " [  2.      27.       0.       0.      10.5   ]\n",
      " [  1.      27.       1.       0.      53.1   ]\n",
      " [  1.      29.       0.       0.     211.3375]\n",
      " [  1.      35.       0.       0.     512.3292]\n",
      " [  1.      36.       1.       0.      78.85  ]\n",
      " [  1.      21.       2.       2.     262.375 ]\n",
      " [  1.      70.       1.       1.      71.    ]\n",
      " [  1.      19.       1.       0.      53.1   ]\n",
      " [  3.       6.       0.       1.      12.475 ]\n",
      " [  1.      33.       0.       0.      86.5   ]\n",
      " [  1.      36.       1.       2.     120.    ]\n",
      " [  1.      51.       1.       0.      77.9583]\n",
      " [  2.      57.       0.       0.      10.5   ]\n",
      " [  1.      43.       0.       1.     211.3375]\n",
      " [  1.      17.       1.       0.      57.    ]\n",
      " [  1.      29.       0.       0.      30.    ]\n",
      " [  1.      46.       0.       0.      79.2   ]\n",
      " [  1.      49.       0.       0.      25.9292]\n",
      " [  1.      11.       1.       2.     120.    ]\n",
      " [  1.      39.       0.       0.       0.    ]\n",
      " [  1.      33.       1.       0.      53.1   ]\n",
      " [  1.      52.       1.       1.      93.5   ]\n",
      " [  3.      27.       0.       1.      12.475 ]\n",
      " [  1.      39.       1.       1.      83.1583]\n",
      " [  1.      16.       0.       1.      39.4   ]\n",
      " [  1.      51.       0.       0.      26.55  ]\n",
      " [  1.      48.       0.       0.      25.9292]\n",
      " [  1.      31.       0.       0.      50.4958]\n",
      " [  1.      47.       1.       1.      52.5542]\n",
      " [  1.      33.       0.       0.       5.    ]\n",
      " [  1.      56.       0.       1.      83.1583]\n",
      " [  1.      19.       0.       0.      30.    ]\n",
      " [  1.      26.       0.       0.      30.    ]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/locke/Dropbox/Programming/Python/RA/realkd.py/realkd/rules use.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-2/mnt/c/Users/locke/Dropbox/Programming/Python/RA/realkd.py/realkd/rules%20use.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m titanic\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mSex\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEmbarked\u001b[39m\u001b[39m'\u001b[39m],  inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-2/mnt/c/Users/locke/Dropbox/Programming/Python/RA/realkd.py/realkd/rules%20use.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m re \u001b[39m=\u001b[39m realkd\u001b[39m.\u001b[39mrules\u001b[39m.\u001b[39mRuleBoostingEstimator(base_learner\u001b[39m=\u001b[39mrealkd\u001b[39m.\u001b[39mrules\u001b[39m.\u001b[39mXGBRuleEstimator(loss\u001b[39m=\u001b[39mrealkd\u001b[39m.\u001b[39mrules\u001b[39m.\u001b[39mlogistic_loss))\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-2/mnt/c/Users/locke/Dropbox/Programming/Python/RA/realkd.py/realkd/rules%20use.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m re\u001b[39m.\u001b[39;49mfit(titanic, survived\u001b[39m.\u001b[39;49mreplace(\u001b[39m0\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39mrules_\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-2/mnt/c/Users/locke/Dropbox/Programming/Python/RA/realkd.py/realkd/rules%20use.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m titanic\n",
      "File \u001b[0;32m/mnt/c/Users/locke/Dropbox/Programming/Python/RA/realkd.py/realkd/rules.py:659\u001b[0m, in \u001b[0;36mRuleBoostingEstimator.fit\u001b[0;34m(self, data, target, feature_names)\u001b[0m\n\u001b[1;32m    657\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrules_(data)\n\u001b[1;32m    658\u001b[0m estimator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_base_learner()\n\u001b[0;32m--> 659\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(data, target, scores, \u001b[39mmax\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m))\n\u001b[1;32m    660\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    661\u001b[0m     \u001b[39mprint\u001b[39m(estimator\u001b[39m.\u001b[39mrule_)\n",
      "File \u001b[0;32m/mnt/c/Users/locke/Dropbox/Programming/Python/RA/realkd.py/realkd/rules.py:529\u001b[0m, in \u001b[0;36mXGBRuleEstimator.fit\u001b[0;34m(self, data, target, scores, verbose)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, data, target, scores\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    516\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[39m    Fits rule to provide best loss reduction on given data\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[39m    (where the baseline prediction scores are either given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m \n\u001b[1;32m    528\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m     obj \u001b[39m=\u001b[39m GradientBoostingObjective(data, target, predictions\u001b[39m=\u001b[39;49mscores, loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss, reg\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreg)\n\u001b[1;32m    530\u001b[0m     q \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39msearch(method\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch, verbose\u001b[39m=\u001b[39mverbose, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_params) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery\n\u001b[1;32m    531\u001b[0m     y \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mopt_weight(q)\n",
      "File \u001b[0;32m/mnt/c/Users/locke/Dropbox/Programming/Python/RA/realkd.py/realkd/rules.py:380\u001b[0m, in \u001b[0;36mGradientBoostingObjective.__init__\u001b[0;34m(self, data, target, predictions, loss, reg)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m g[order]\n\u001b[1;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m h[order]\n\u001b[0;32m--> 380\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49miloc[order]\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    381\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39miloc[order]\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    382\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(target)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "# TODO: Delete this file before merging to development\n",
    "\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import realkd.rules\n",
    "importlib.reload(realkd.rules)\n",
    "\n",
    "titanic = pd.read_csv('../datasets/titanic/train.csv')\n",
    "titanic.dropna(inplace=True)\n",
    "survived = titanic.Survived\n",
    "titanic.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin', 'Survived'], inplace=True)\n",
    "titanic.drop(columns=['Sex', 'Embarked'],  inplace=True)\n",
    "re = realkd.rules.RuleBoostingEstimator(base_learner=realkd.rules.XGBRuleEstimator(loss=realkd.rules.logistic_loss))\n",
    "re.fit(titanic, survived.replace(0, -1)).rules_\n",
    "titanic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
